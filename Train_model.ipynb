{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model import model, loss_function\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataLoader import DataLoader\n",
    "top_k = 3000\n",
    "data_path = '/data/Niklas/Flickr30k'\n",
    "dataLoader = DataLoader(data_path)\n",
    "dataLoader.convert_to_dataset(top_k)\n",
    "if len(dataLoader.tokenizer.index_word) < top_k:\n",
    "    top_k = len(dataLoader.tokenizer.index_word) - 1 # If our dictonary is less than top_k. -1 because <unk> is included\n",
    "    dataLoader.top_k = top_k\n",
    "max_length = dataLoader.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 116000\n",
      "Validation size: 29000\n",
      "Max length: 80\n",
      "Top k: 3000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training size: {0}\".format(len(dataLoader.cap_train)))\n",
    "print(\"Validation size: {0}\".format(len(dataLoader.cap_val)))\n",
    "print(\"Max length: {0}\".format(max_length))\n",
    "print(\"Top k: {0}\".format(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create & train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 16, Steps per epochs: 7250\n"
     ]
    }
   ],
   "source": [
    "from create_dataset import create_dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "image_shape = (299,299,3)\n",
    "embedding_matrix = None\n",
    "embedding_dim = 512\n",
    "lstm_units = embedding_dim\n",
    "\n",
    "buffer_size = 1000\n",
    "batch_size = 16\n",
    "train_dataset = create_dataset(dataLoader.img_name_train, dataLoader.cap_train, batch_size, buffer_size)\n",
    "val_dataset = create_dataset(dataLoader.img_name_val, dataLoader.cap_val, batch_size, buffer_size)\n",
    "num_batches = int(len(dataLoader.cap_train)/batch_size)\n",
    "\n",
    "print(\"Batches: {0}, Steps per epochs: {1}\".format(batch_size, num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Layer model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Train for 2416 steps, validate for 604 steps\n",
      "Epoch 1/10\n",
      "INFO:tensorflow:batch_all_reduce: 9 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2').\n",
      "INFO:tensorflow:batch_all_reduce: 9 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2').\n",
      "2415/2416 [============================>.] - ETA: 0s - loss: 0.6394"
     ]
    }
   ],
   "source": [
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./Saved_models/best_model',\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "batch_size *= mirrored_strategy.num_replicas_in_sync\n",
    "train_batches = int(len(dataLoader.cap_train)/batch_size)\n",
    "val_batches = int(len(dataLoader.cap_val)/batch_size)\n",
    "with mirrored_strategy.scope():\n",
    "    m = model(image_shape, embedding_dim, lstm_units, top_k, max_length)\n",
    "    m.compile(optimizer = tf.keras.optimizers.Adam(), loss = loss_function)\n",
    "    history = m.fit(train_dataset, \n",
    "                    epochs = 10,\n",
    "                    steps_per_epoch = train_batches,\n",
    "                    shuffle=True,\n",
    "                    callbacks = [model_checkpoint_callback],\n",
    "                    validation_data = val_dataset,\n",
    "                    validation_steps = val_batches)\n",
    "    m.save_weights('./Models/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(m.history.history['loss'])\n",
    "plt.plot(m.history.history['val_loss'])\n",
    "plt.title(\"Traning\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend([\"traning\", \"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model(image_shape, embedding_dim, lstm_units, top_k, max_length)\n",
    "m.load_weights('./Saved_models/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import read_img\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = dataLoader.img_name_train[0]\n",
    "cap = dataLoader.org_data[sample]\n",
    "img = read_img(sample)\n",
    "\n",
    "img_1 = tf.io.read_file(sample)\n",
    "img_1 = tf.image.decode_jpeg(img_1, channels=3)\n",
    "plt.imshow(img_1)\n",
    "\n",
    "result = m.predict(img[None,...], dataLoader.tokenizer)\n",
    "print(' '.join(result))\n",
    "[print(c) for c in cap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import read_img\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = dataLoader.img_name_val[0]\n",
    "cap = dataLoader.org_data[sample]\n",
    "img = read_img(sample)\n",
    "\n",
    "img_1 = tf.io.read_file(sample)\n",
    "img_1 = tf.image.decode_jpeg(img_1, channels=3)\n",
    "plt.imshow(img_1)\n",
    "\n",
    "result = m.predict(img[None,...], dataLoader.tokenizer)\n",
    "print(' '.join(result))\n",
    "[print(c) for c in cap]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on own imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset import read_img\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = \"\"\n",
    "cap = dataLoader.org_data[sample]\n",
    "img = read_img(sample)\n",
    "\n",
    "img_1 = tf.io.read_file(sample)\n",
    "img_1 = tf.image.decode_jpeg(img_1, channels=3)\n",
    "plt.imshow(img_1)\n",
    "\n",
    "result = m.predict(img[None,...], dataLoader.tokenizer)\n",
    "print(' '.join(result))\n",
    "[print(c) for c in cap]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
